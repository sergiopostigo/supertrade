{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Persistent Loader - HDFS + Parquet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from project_settings import env\n",
    "from database_settings import hdfs_utilities as hdfs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion finished! headings file added to HDFS\n"
     ]
    }
   ],
   "source": [
    "# Get the path of the headings file\n",
    "file = env.TEMPORAL_LANDING_FOLDER+'/headings/NANDINA.txt'\n",
    "\n",
    "# Parse the file and convert it into a Dataframe\n",
    "with open(file, 'r') as f:\n",
    "    file_lines = f.readlines()\n",
    "    file_lines = [string.rstrip('\\t\\n') for string in file_lines][1:]\n",
    "    file_lines = [string.split('\\t') for string in file_lines]\n",
    "    file_lines = [[element for element in inner_list if element.strip()] for inner_list in file_lines]\n",
    "# Convert to dataframe\n",
    "headings = pd.DataFrame(file_lines)\n",
    "# Convert the column names into strings\n",
    "headings.columns = headings.columns.astype(str)\n",
    "# Add the loading date column\n",
    "headings['LOAD_DATE'] = datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "# Create a parquet file\n",
    "# Convert the dataframe into a pyarrow table\n",
    "headings = pa.Table.from_pandas(headings)\n",
    "# Generate the parquet file in the same folder than the original headings file\n",
    "parquet_writer = pq.ParquetWriter(os.path.dirname(file) + '/headings' + '.parquet', headings.schema)\n",
    "parquet_writer.write_table(headings)\n",
    "parquet_writer.close()\n",
    "\n",
    "# Define the directory in HDFS to store the files\n",
    "hdfs_directory = '/thesis/peru/headings/'\n",
    "# Add the files\n",
    "result = hdfs.add_file_to_hdfs(os.path.dirname(file)+'/headings.parquet', hdfs_directory, log_context='historical')\n",
    "if result == 0:\n",
    "    print('Ingestion finished! headings file added to HDFS')\n",
    "else:\n",
    "    print('Ingestion of headings in HDFS failed!')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
