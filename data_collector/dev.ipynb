{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Collector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max, to_date\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def spark_session():\n",
    "\n",
    "    # Define the HDFS configuration\n",
    "    hdfs_uri = \"hdfs://localhost:9000/\"\n",
    "\n",
    "    # Create the Spark session\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"hdfs_query\") \\\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", hdfs_uri) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    return spark\n",
    "\n",
    "spark = spark_session()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "def get_spark_df(spark_session, query=None, file_path=None):\n",
    "\n",
    "    # Ensure that whether query or file_path is provided\n",
    "    if query is None and file_path is None:\n",
    "        raise ValueError(\"Either 'query' or 'file_path' must be provided.\")\n",
    "    if query is not None and file_path is not None:\n",
    "        raise ValueError(\"Only one of 'query' or 'file_path' should be provided.\")\n",
    "\n",
    "    # If a query was provided, return a df with its result\n",
    "    if query:\n",
    "        df = spark.sql(query.format(query))\n",
    "\n",
    "    # If a file_path was provided, return a df with the content of that file\n",
    "    elif file_path:\n",
    "        df = spark_session.read.parquet(file_path)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent date: 2023-03-12\n"
     ]
    }
   ],
   "source": [
    "path = '/thesis/peru/exports/*.parquet'\n",
    "df = get_spark_df(spark_session=spark,file_path=path)\n",
    "df = df.withColumn(\"date\", to_date(col(\"BATCH_WEEK\").substr(3,6), \"ddMMyy\"))\n",
    "max_date = df.select(max(\"date\")).first()[0]\n",
    "print(\"Most recent date:\", max_date)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
