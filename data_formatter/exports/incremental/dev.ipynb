{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exports Incremental Formatting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from database_settings import spark_utilities\n",
    "from pyspark.sql.functions import col, lpad, concat_ws, regexp_replace, trim, to_date\n",
    "from database_settings import postgres_utilities\n",
    "from data_formatter import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Get headings to work\n",
    "headings = utilities.get_headings()\n",
    "headings_filter = r\"^(\"+ \"|\".join(headings) + \")\" # filter out headings that aren't in the list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connected successfully\n",
      "Most recent batch: 2023-04-16\n"
     ]
    }
   ],
   "source": [
    "# Get the most recent batch week code from the data in the Formatted Zone\n",
    "\n",
    "# Establish the connection with the database\n",
    "conn = postgres_utilities.connect()\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Query to get the most recent week range\n",
    "query = \"\"\"\n",
    "SELECT to_date(SUBSTRING(BATCH_WEEK, 3, 8), 'DDMMYY') as LAST_DAY\n",
    "FROM peru_exports\n",
    "ORDER BY LAST_DAY DESC\n",
    "LIMIT 1;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "cur.execute(query)\n",
    "last_day_of_data = cur.fetchone()[0]\n",
    "print('Most recent batch: {}'.format(last_day_of_data))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Get new data from the Persistent Zone\n",
    "print(\"Performing query on Persistent Zone...\")\n",
    "df = spark_utilities.get_spark_df('peru_exports')\\\n",
    "    .select('PART_NANDI','VPESNET', 'VPESBRU', 'VFOBSERDOL', 'CPAIDES','NDOC','FEMB','DCOM','DMER2','DMER3','DMER4','DMER5','BATCH_WEEK')\\\n",
    "    .withColumn(\"heading\", lpad(col(\"PART_NANDI\").cast(\"string\"), 10, \"0\")) \\\n",
    "    .filter(col(\"heading\").rlike(headings_filter))\\\n",
    "    .filter(to_date(col(\"BATCH_WEEK\").substr(-6, 6), 'ddMMyy') > last_day_of_data)\\\n",
    "    .withColumn(\"description\", concat_ws(\" \", col(\"DCOM\"), col(\"DMER2\"), col(\"DMER3\"), col(\"DMER4\"), col(\"DMER5\"))) \\\n",
    "    .withColumn(\"description\", regexp_replace(col(\"description\"), \"NaN\", \"\")) \\\n",
    "    .withColumn(\"description\", trim(col(\"description\")))\\\n",
    "    .withColumn(\"NDOC\", regexp_replace(col(\"NDOC\"), \"No Disponib\", \"unknown\")) \\\n",
    "    .withColumnRenamed('NDOC', 'exp_id')\\\n",
    "    .withColumnRenamed('VPESNET', 'net_weight')\\\n",
    "    .withColumnRenamed('VPESBRU', 'gross_weight')\\\n",
    "    .withColumnRenamed('VFOBSERDOL', 'value_usd')\\\n",
    "    .withColumnRenamed('CPAIDES', 'country')\\\n",
    "    .withColumnRenamed('FEMB', 'boarding_date')\\\n",
    "    .withColumnRenamed('BATCH_WEEK', 'batch_week')\\\n",
    "    .select('heading','exp_id','net_weight','gross_weight','value_usd','country','boarding_date',\"description\",'batch_week')\\\n",
    "    .toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [heading, exp_id, net_weight, gross_weight, value_usd, country, boarding_date, description, batch_week]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>heading</th>\n      <th>exp_id</th>\n      <th>net_weight</th>\n      <th>gross_weight</th>\n      <th>value_usd</th>\n      <th>country</th>\n      <th>boarding_date</th>\n      <th>description</th>\n      <th>batch_week</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connected successfully\n",
      "DataFrame sent to PostgreSQL table successfully!\n"
     ]
    }
   ],
   "source": [
    "# Add it to the formatted zone\n",
    "\n",
    "# Establish the connection with the database\n",
    "conn = postgres_utilities.connect()\n",
    "engine = postgres_utilities.engine()\n",
    "\n",
    "# Rename the columns and write in the database\n",
    "try:\n",
    "    df.to_sql('peru_exports', engine, if_exists='append', index=False)\n",
    "    print(\"Data sent to Formatted Zone successfully: {} rows added\".format(len(df)))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error while sending data to Formatted Zone: {e}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
